---
title: Unsupervised Neural Machine Translation
summary: Unsupervised Neural Machine Translation amongst low resource Indian languages.
tags:
- Deep Learning
date: "2020-05-25T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: ""

#image:
#  caption: Photo by rawpixel on Unsplash
#  focal_point: Smart

#links:
#- icon: twitter
#  icon_pack: fab
#  name: Follow
#  url: https://twitter.com/georgecushen
#url_code: "https://github.com/niksarrow/cst"
#url_pdf: "https://www.aclweb.org/anthology/2020.iwslt-1.22/"
#url_slides: ""
#url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---

Progress in Unsupervised MT has been impressive where certain models are able to achieve
performance near its supervised counterpart in particular settings. Unsupervised Neural
Machine Translation performance relies on the intersection between the training and test
datasets. The state-of-the art architectures fail on the real low-resource scenarios. Evaluating current UNMT architectures on truly low-resource datasets shows that it struggles
in realistic scenarios. It is an open question that the three pillars of unsupervised MT
are sufficient and necessary or there is a need of more? In this work, we found out that
the current UNMT models fail when the datasets are dissimilar amongst Indian language
pairs. It is highly encouraged to do extensive evaluation of these architectures in all broad
data scenarios and opening pursuits for research in each one of them.
